{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f32d4f8-3c70-4aad-b947-46fa27fed181",
   "metadata": {},
   "source": [
    "# RISQUE DE DÉFAILLANCE DU CRÉDIT IMMOBILIER\n",
    "\n",
    "- La concaténation des données des différents csv, le cleaning et le feature engineering (calculs de nouvelles variables et one hot encoder) a été extrait d'un kernel d'un concours kaggle. Cette personne avait réalisé un super travail en étant bien classée avec un très bon score pour son modèle.\n",
    "- Le but ici est de récupérer juste sa partie de travail avant le preprocessing (imputing nan / scaling etc..) et de s'occuper des 4 prochaines parties:\n",
    "    * Modélisation et Optimisation\n",
    "    * Feature importance globale et locale\n",
    "    * Mise en production d'une api (mise en place avec Flask, déployée sur Heroku) prédisant l'accord d'un prêt ou non avec score\n",
    "    * Mise en production d'un dashboard (mis en place et déployé sur streamlit) qui appelle l'api\n",
    "\n",
    "### La plupart des caractéristiques sont créées en appliquant les fonctions min, max, mean, sum et var à des tableaux groupés. \n",
    "\n",
    "### Les idées clés suivantes ont été utilisées :\n",
    "- Diviser ou soustraire des caractéristiques importantes pour obtenir des taux (comme l'annuité et le revenu).\n",
    "- Dans les données du bureau : créer des caractéristiques spécifiques pour les crédits actifs et les crédits fermés.\n",
    "- Dans les demandes précédentes : créer des caractéristiques spécifiques pour les demandes approuvées et refusées.\n",
    "- Encodage unique pour les caractéristiques catégorielles\n",
    "\n",
    "### Toutes les tables sont jointes en utilisant la clé SK_ID_CURR (ID du client)\n",
    "\n",
    "### Variable cible: \n",
    "- 1 - client ayant des difficultés de paiement : il/elle a eu un retard de paiement de plus de X jours sur au moins une des Y premières échéances du prêt dans notre échantillon\n",
    "- 0 - tous les autres cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d340d198-a364-4ecb-9b18-9070ba6df552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d81f239-67a9-4f3b-8790-ea95edb033b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199230b6-5b88-4f82-887c-3d519064fc6e",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738d47d1-4331-432a-a862-3a03287ea981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Label encoding for categorical columns with factorize\n",
    "def label_encoder(df):\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    binary_categorical_columns = []\n",
    "    for column in categorical_columns:\n",
    "        if len(df[column].unique()) == 2:\n",
    "            binary_categorical_columns.append(column)\n",
    "    for bin_feature in binary_categorical_columns:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature], sort=True)\n",
    "    return df\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7bc673-0ca9-4e81-a978-f8e74385010a",
   "metadata": {},
   "source": [
    "# Cleaning and Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a53eb2-6cb7-456c-820d-714e123eab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess application_train.csv\n",
    "def application_train(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('input/application_train.csv', nrows= num_rows)\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    df = label_encoder(df)\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    df = df.set_index('SK_ID_CURR')\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5caa5c-8115-48ca-8fdf-715bf8690986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('input/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('input/bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c0bab7-eff1-4287-b107-0a97f8d956ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('input/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts (POS_MONTHS_BALANCE_SIZE)\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9407ea02-5dc0-4d85-ab20-521d1346e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('input/installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47c54fb-6c15-463f-9258-31b0f8b7013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv('input/credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9820a1fc-4337-49c8-89cd-87094b76e02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 10.3 s, total: 1min 22s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_app_train = application_train()\n",
    "df_bureau_and_balance = bureau_and_balance()\n",
    "df_pos_cash = pos_cash()\n",
    "df_installments_payments = installments_payments()\n",
    "df_credit_card_balance = credit_card_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42167569-4cc7-4680-bb19-bb5c9e7f3364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307507, 245), (305811, 116), (337252, 18), (339587, 26), (103558, 141))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app_train.shape, df_bureau_and_balance.shape, df_pos_cash.shape, df_installments_payments.shape, df_credit_card_balance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659754e4-f884-4eba-aa6f-c59c62679af8",
   "metadata": {},
   "source": [
    "# Join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf252cad-6ec3-4f79-a89a-0980fdcc4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_app_train \\\n",
    ".join(df_bureau_and_balance, how='left', on='SK_ID_CURR') \\\n",
    ".join(df_pos_cash, how='left', on='SK_ID_CURR') \\\n",
    ".join(df_installments_payments, how='left', on='SK_ID_CURR') \\\n",
    ".join(df_credit_card_balance, how='left', on='SK_ID_CURR') \\\n",
    ".reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b9a5de4-d8d9-4c7d-88fe-240b34a567e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and columns where too many Nan / Replace infini values by Nan values\n",
    "def drop_columns_too_many_Nan(df): \n",
    "    df = df.drop_duplicates()\n",
    "    total_nan_pourcentage = df.isnull().sum()/df.shape[0]\n",
    "    features_inf = total_nan_pourcentage[total_nan_pourcentage.values<0.50]\n",
    "    index_features_inf = list(features_inf.index)\n",
    "    df_clean = df[index_features_inf]\n",
    "    df_clean.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "    return df_clean\n",
    "\n",
    "df_final = drop_columns_too_many_Nan(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3c0bfd-f7e5-4f95-b005-06fa147c3ee3",
   "metadata": {},
   "source": [
    "- On sauvegarde le Dataframe final avant le proproccessing, modelling and optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38215492-c8be-43ee-b3e1-c160d5d16933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('data_clean/df_final_before_preprocessing.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
